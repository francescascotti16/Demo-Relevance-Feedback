{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from f_display_and_feedback import *\n",
    "from f_pichunter_star import *\n",
    "from f_polyquery_msed_logscale import *\n",
    "from f_polyadic_sed import *\n",
    "from f_process_data import *\n",
    "from f_rocchio import *\n",
    "from f_svm import *\n",
    "from f_process_data import *\n",
    "import h5py\n",
    "import tqdm \n",
    "import random\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francescascotti/dev/interfaccia/f_process_data.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_visione_mapping = pd.read_csv(csv_mapping_path, dtype={'column_name': str})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdf5_file_path= '/home/francescascotti/dev/interfaccia/indexed_file.h5'\n",
    "hdf5_file = h5py.File(hdf5_file_path, 'r')\n",
    "features = hdf5_file['indexed_data/data'][:]\n",
    "ids = hdf5_file['indexed_data/ids'][:]\n",
    "hdf5_file.close()\n",
    "      \n",
    "# pickle load  /home/francescascotti/dev/interfaccia/indexed_data_logistic \n",
    "     \n",
    "with open('/home/francescascotti/dev/interfaccia/indexed_data_logistic', 'rb') as file:\n",
    "    features_logistic = pickle.load(file)\n",
    "  \n",
    "    \n",
    "shot_labels_query, queries =  process_data( '/home/francescascotti/data/avsGT/avs_gt_visione_mapping.csv','/home/francescascotti/data/avsGT/avs_gt_visione_mapping_query_judgment.csv',threshold_1=200, threshold_0=1400)\n",
    "features = pd.DataFrame(features).transpose()\n",
    "features.columns = [id.decode() if isinstance(id, bytes) else id for id in ids]\n",
    "features_logistic = pd.DataFrame(features_logistic).transpose()\n",
    "features_logistic.columns = features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_10_labeled_positive = []  # Stores 10 positive samples per query\n",
    "lista_10_labeled_negative = []  # Stores 10 negative samples per query\n",
    "lista_total = []  # Stores both positive and negative samples per query\n",
    "\n",
    "for query in queries:\n",
    "    seed = 42 \n",
    "    \n",
    "    # Determine the sample size (max 10,000 or the number of available samples)\n",
    "    sample_size = min(10000, len([el[0] for el in shot_labels_query.keys() if el[1] == query]))\n",
    "    \n",
    "    # Extract feature IDs that belong to the current query\n",
    "    features_id_accepted = random.sample(\n",
    "        [el[0] for el in shot_labels_query.keys() if el[1] == query], sample_size\n",
    "    )\n",
    "    \n",
    "    # Select 5 positive and 5 negative samples randomly from the accepted features\n",
    "    positive_first_display = random.sample(\n",
    "        [i for i in features_id_accepted if shot_labels_query[(i, query)] == 1], 5\n",
    "    ) \n",
    "    negative_first_display = random.sample(\n",
    "        [i for i in features_id_accepted if shot_labels_query[(i, query)] == 0], 5\n",
    "    ) \n",
    "    \n",
    "    # Append the selected samples to the corresponding lists\n",
    "    lista_10_labeled_positive.append(positive_first_display)\n",
    "    lista_10_labeled_negative.append(negative_first_display)\n",
    "    lista_total.append(positive_first_display + negative_first_display)\n",
    "\n",
    "# Save positive and negative labeled lists as JSON files\n",
    "with open('/home/francescascotti/dev/interfaccia/lista_10_labeled_positive.json', 'w') as f:\n",
    "    json.dump(lista_10_labeled_positive, f)\n",
    "\n",
    "with open('/home/francescascotti/dev/interfaccia/lista_10_labeled_negative.json', 'w') as f:\n",
    "    json.dump(lista_10_labeled_negative, f)\n",
    "\n",
    "# Count the total number of occurrences and track unique feature IDs\n",
    "count = 0\n",
    "duplicates = []  # Stores unique feature IDs\n",
    "ids_to_keep = []  # Stores all selected feature IDs\n",
    "\n",
    "for el in lista_total:\n",
    "    for aa in el:\n",
    "        ids_to_keep.append(aa)  # Append feature ID to tracking list\n",
    "        if aa not in duplicates:\n",
    "            duplicates.append(aa)  # Add to unique list if not already present\n",
    "        count += 1  # Increment total count\n",
    "\n",
    "# Extract dataset using selected feature IDs\n",
    "dataset_800 = features[ids_to_keep]\n",
    "\n",
    "target_columns = 10000  # Target number of columns in the final dataset\n",
    "current_columns = len(dataset_800.columns)  # Current number of columns\n",
    "columns_to_add = target_columns - current_columns  # Calculate missing columns\n",
    "\n",
    "# If additional columns are needed, randomly select them from available features\n",
    "if columns_to_add > 0:\n",
    "    new_features_ids = random.sample(\n",
    "        [el for el in features.columns if el not in dataset_800.columns], columns_to_add\n",
    "    )\n",
    "    new_features = features[new_features_ids]  # Extract the additional features\n",
    "    \n",
    "    # Concatenate the new features to the dataset\n",
    "    dataset_800 = pd.concat([dataset_800, new_features], axis=1)\n",
    "\n",
    "# Save the final dataset as a CSV file\n",
    "dataset_800.to_csv('/home/francescascotti/dev/interfaccia/data_10000_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Query:  1591\n",
      "Data preparation time:  0:00:00.122084\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1592\n",
      "Data preparation time:  0:00:00.123514\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1593\n",
      "Data preparation time:  0:00:00.121902\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1594\n",
      "Data preparation time:  0:00:00.122178\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1595\n",
      "Data preparation time:  0:00:00.121326\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1597\n",
      "Data preparation time:  0:00:00.121951\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1598\n",
      "Data preparation time:  0:00:00.122112\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1599\n",
      "Data preparation time:  0:00:00.121726\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1600\n",
      "Data preparation time:  0:00:00.122498\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1601\n",
      "Data preparation time:  0:00:00.127621\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1602\n",
      "Data preparation time:  0:00:00.124328\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1603\n",
      "Data preparation time:  0:00:00.121593\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1604\n",
      "Data preparation time:  0:00:00.123050\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1605\n",
      "Data preparation time:  0:00:00.121404\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1606\n",
      "Data preparation time:  0:00:00.121864\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1607\n",
      "Data preparation time:  0:00:00.126981\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1608\n",
      "Data preparation time:  0:00:00.121061\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1609\n",
      "Data preparation time:  0:00:00.122118\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1610\n",
      "Data preparation time:  0:00:00.121336\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1661\n",
      "Data preparation time:  0:00:00.121919\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1663\n",
      "Data preparation time:  0:00:00.121851\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1664\n",
      "Data preparation time:  0:00:00.125579\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1665\n",
      "Data preparation time:  0:00:00.122198\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1666\n",
      "Data preparation time:  0:00:00.137209\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1667\n",
      "Data preparation time:  0:00:00.121840\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1668\n",
      "Data preparation time:  0:00:00.125703\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1669\n",
      "Data preparation time:  0:00:00.128994\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1670\n",
      "Data preparation time:  0:00:00.127456\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1671\n",
      "Data preparation time:  0:00:00.121171\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1672\n",
      "Data preparation time:  0:00:00.121947\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1673\n",
      "Data preparation time:  0:00:00.141311\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1675\n",
      "Data preparation time:  0:00:00.125257\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1676\n",
      "Data preparation time:  0:00:00.122729\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1677\n",
      "Data preparation time:  0:00:00.121756\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1678\n",
      "Data preparation time:  0:00:00.125958\n",
      "--------------------\n",
      "--------------------\n",
      "Query:  1680\n",
      "Data preparation time:  0:00:00.128126\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Remove duplicate queries by converting the list to a set and back to a list\n",
    "queries = list(set(queries))\n",
    "\n",
    "# Load dataset containing 10,000 data points\n",
    "data_10000_df = pd.read_csv('/home/francescascotti/dev/interfaccia/data_10000_df.csv')\n",
    "\n",
    "# Select only relevant columns for logistic regression features\n",
    "data_10000_df_logistic = features_logistic[data_10000_df.columns]\n",
    "\n",
    "# Load labeled positive and negative examples from JSON files\n",
    "with open('/home/francescascotti/dev/interfaccia/lista_10_labeled_positive.json', 'r') as f:\n",
    "    lista_10_labeled_positive = json.load(f)\n",
    "with open('/home/francescascotti/dev/interfaccia/lista_10_labeled_negative.json', 'r') as f:\n",
    "    lista_10_labeled_negative = json.load(f)\n",
    "\n",
    "# Initialize lists to store execution times of different methods\n",
    "rocchio_time, svm_time, pichunter_time, pichunter_star_time, polyadic_sed_time, polyadic_msed_time = [], [], [], [], [], []\n",
    "\n",
    "# Iterate over each query in the list\n",
    "for i in range(len(queries)):\n",
    "    start_time = datetime.now()  # Start timing data preparation\n",
    "    query = queries[i]  # Current query\n",
    "    seed = 42  # Set random seed for reproducibility\n",
    "    print(\"--------------------\")\n",
    "    print(\"Query: \", query)\n",
    "    \n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Retrieve positive and negative samples for initial display\n",
    "    positive_first_display = lista_10_labeled_positive[i]\n",
    "    negative_first_display = lista_10_labeled_negative[i]\n",
    "    \n",
    "    # Create the initial display dataset with labeled examples\n",
    "    display_df_1 = pd.concat([features[positive_first_display], features[negative_first_display]], axis=1)\n",
    "    \n",
    "    # Randomly select 90 additional features for display\n",
    "    new_features_id = random.sample([el for el in features.columns], 90)\n",
    "    display_df_finale = pd.concat([display_df_1, features[new_features_id]], axis=1)\n",
    "    display_df_final_logistic = features_logistic[display_df_finale.columns]\n",
    "    \n",
    "    # Ensure the dataset contains the expected number of features\n",
    "    if len(list(set(display_df_finale.columns))) != 100:\n",
    "        print('Error: Incorrect number of features in display dataset')\n",
    "    if len(list(set(data_10000_df.columns))) != 10000:\n",
    "        print('Error: Incorrect number of features in full dataset')\n",
    "    \n",
    "    end_time = datetime.now()  # End timing data preparation\n",
    "    print(\"Data preparation time: \", end_time - start_time)\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    ### Run different relevance feedback methods and store execution times ###\n",
    "    \n",
    "    # ROCCHIO Method\n",
    "    _, _, _, time_of_search_rocchio, _, _ = rocchio_single_step(\n",
    "        data_10000_df, display_df_finale, positive_first_display,\n",
    "        negative_first_display, alpha=0.75, beta=1, gamma=0.75,\n",
    "        fun_name=\"euclidean\", initial_query=None\n",
    "    )\n",
    "    rocchio_time.append(time_of_search_rocchio)\n",
    "    \n",
    "    # SVM Method\n",
    "    _, _, time_of_search_svm, _, _, _ = svm_single_step(\n",
    "        data_10000_df, display_df_finale, positive_first_display,\n",
    "        negative_first_display, initial_scores=None, alpha=0.75, beta=0.25\n",
    "    )\n",
    "    svm_time.append(time_of_search_svm)\n",
    "    \n",
    "    # PICHUNTER_STAR Method\n",
    "    _, _, time_of_search_pic_star, _, _ = pichunter_single_step(\n",
    "        data_10000_df, display_df_finale, positive_first_display, negative_first_display,\n",
    "        fun_name=\"softmin\", initial_prob=0, temperature=82.10553\n",
    "    )\n",
    "    pichunter_star_time.append(time_of_search_pic_star)\n",
    "    \n",
    "    # PICHUNTER Method\n",
    "    _, _, time_of_search_pic, _, _ = pichunter_single_step(\n",
    "        data_10000_df, display_df_finale, positive_first_display, [],\n",
    "        fun_name=\"softmin\", initial_prob=0, temperature=82.10553\n",
    "    )\n",
    "    pichunter_time.append(time_of_search_pic)\n",
    "    \n",
    "    # POLYADIC SED Method\n",
    "    _, _, _, _, time_of_search_sed, _, _ = poly_sed_logscale_single_step(\n",
    "        data_10000_df_logistic, display_df_final_logistic,\n",
    "        positive_first_display, negative_first_display,\n",
    "        precomputed_dict_initial=None, alpha=0.75, beta=1, gamma=0.75,\n",
    "        initial_query=None, initial_scores=None, entropy_dict=None\n",
    "    )\n",
    "    polyadic_sed_time.append(time_of_search_sed)\n",
    "    \n",
    "    # POLYADIC MSED Method\n",
    "    _, _, _, _, time_of_search_msed, _, _ = poly_msed_logscale_single_step(\n",
    "        data_10000_df_logistic, display_df_final_logistic,\n",
    "        positive_first_display, negative_first_display,\n",
    "        precomputed_dict_initial=None, alpha=0.75, beta=1, gamma=0.75,\n",
    "        initial_query=None, initial_scores=None, entropy_dict=None\n",
    "    )\n",
    "    polyadic_msed_time.append(time_of_search_msed)\n",
    "\n",
    "# Uncomment the following lines if you need to save the execution times to JSON files\n",
    "# with open('/home/francescascotti/dev/interfaccia/time_dict_pichunter.json', 'w') as file:\n",
    "#     json.dump(time_dict_pichunter, file, indent=4)\n",
    "# with open('/home/francescascotti/dev/interfaccia/time_dict_svm.json', 'w') as file:\n",
    "#     json.dump(time_dict_svm, file, indent=4)\n",
    "# with open('/home/francescascotti/dev/interfaccia/time_dict_rocchio.json', 'w') as file:\n",
    "#     json.dump(time_dict_rocchio, file, indent=4)\n",
    "# with open('/home/francescascotti/dev/interfaccia/time_dict_polyadic2.json', 'w') as file:\n",
    "#     json.dump(time_dict_sed_logscale, file, indent=4)\n",
    "# with open('/home/francescascotti/dev/interfaccia/time_dict_msed.json', 'w') as file:\n",
    "#     json.dump(time_dict_msed_logscale, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean rocchio time: 0.052747\n",
      "mean svm time: 0.045806\n",
      "mean pichunter time: 0.09063\n",
      "mean pichunter star time: 0.095069\n",
      "mean polyadic sed time: 2.819505\n",
      "mean polyadic msed time: 2.763181\n"
     ]
    }
   ],
   "source": [
    "print('mean rocchio time:', np.mean(rocchio_time).total_seconds())\n",
    "print('mean svm time:', np.mean(svm_time).total_seconds())\n",
    "print('mean pichunter time:', np.mean(pichunter_time).total_seconds())\n",
    "print('mean pichunter star time:', np.mean(pichunter_star_time).total_seconds())\n",
    "print('mean polyadic sed time:', np.mean(polyadic_sed_time).total_seconds())\n",
    "print('mean polyadic msed time:', np.mean(polyadic_msed_time).total_seconds())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
